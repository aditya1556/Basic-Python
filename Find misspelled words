f = open("story.txt", 'r')
story_string = f.read()
clean_chars = [",", ".", "'", ";", "\n"]

# this function cleans special characters
def clean_text(text_string):
    cleaned_string = text_string.replace(",","")
    cleaned_string = cleaned_string.replace(".","")
    cleaned_string = cleaned_string.replace("'", "")
    cleaned_string = cleaned_string.replace(";", "")
    cleaned_string = cleaned_string.replace("\n", "")
    cleaned_string = cleaned_string.lower()
    return(cleaned_string)
cleaned_story = ""
# this function cleans the characters and returns everything in lowercase
def clean_text(text_string, special_characters):
    cleaned_string = text_string 
    for special in special_characters:
        cleaned_string = cleaned_string.replace(special, "")
    cleaned_string = cleaned_string.lower()
    return(cleaned_string)
# this function tokenize every word in the text    
def tokenize(text_string, special_characters):
       cleaned_story = clean_text(text_string, special_characters)
       story_tokens = cleaned_story.split(" ")
       return(story_tokens)
misspelled_words = []

tokenized_story = tokenize(story_string, clean_chars)
tokenized_vocabulary = tokenize(vocabulary, clean_chars)

# finds all the misspelled words
for words in tokenized_story:
    if words not in tokenized_vocabulary:
   
        misspelled_words.append(words)
print(misspelled_words)  
